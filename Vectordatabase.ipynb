import os
import requests
import deeplake
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings

# 1. Download the text file from the given URL
file_url = "https://sherlock-holm.es/stories/plain-text/stud.txt"
response = requests.get(file_url)

if response.status_code != 200:
    raise Exception(f"Failed to download the file. Status code: {response.status_code}")

# 2. Save the downloaded content to a local file
file_path = 'downloaded_example.txt'
with open(file_path, 'w', encoding='utf-8') as f:
    f.write(response.text)

print(f"File downloaded and saved to {file_path}")

# 3. Load and process the text file using LangChain's TextLoader
loader = TextLoader(file_path)
documents = loader.load()

# 4. Split the document into manageable chunks
text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

print(f"Document split into {len(docs)} chunks.")

# 5. Retrieve the OpenAI API key from environment variables
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_KEY:
    raise EnvironmentError("OPENAI_API_KEY is not set. Please provide a valid API key.")

# 6. Initialize the OpenAI embeddings
embeddings = OpenAIEmbeddings(api_key=OPENAI_KEY, model="text-embedding-ada-002")

# 7. Define the Deep Lake dataset path and organization ID
activeloop_org_id = "amirhosseinazami"
activeloop_dataset_name = "RAG_1"
dataset_path = f"hub://{activeloop_org_id}/{activeloop_dataset_name}"

# 8. Create a new Deep Lake dataset and specify the tensors
db = deeplake.empty(dataset_path, overwrite=True, public=True)

# Define the schema for the dataset
db.create_tensor("text", htype="text", dtype=str)
db.create_tensor("embedding", htype="embedding", dtype=float)

print(f"Deep Lake dataset initialized at {dataset_path}")

# 9. Process and store the document embeddings in the Deep Lake dataset
for i, doc in enumerate(docs):
    try:
        # Generate embeddings for each document chunk
        doc_embedding = embeddings.embed_query(doc.page_content)
        if doc_embedding is None:
            raise ValueError(f"Failed to generate embedding for chunk {i}.")

        # Add the document content and embeddings to the Deep Lake dataset
        db.append({
            "text": doc.page_content,
            "embedding": doc_embedding
        })

        print(f"Stored chunk {i} successfully.")

    except Exception as e:
        print(f"Error generating embedding for chunk {i}: {str(e)}")

print("Dataset created successfully with document embeddings.")
