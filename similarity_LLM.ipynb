from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from langchain.llms import OpenAI    
from langchain.prompts import PromptTemplate

# Step 1: Get the query from the user
query = input("Enter your query: ")

# Step 2: Embed the query using the embeddings model
embedded_query = embeddings.embed_query(query)

# Step 3: Compute cosine similarity between query and stored embeddings
cosine_sim_matrix = cosine_similarity([embedded_query], db["embedding"].numpy())

# Step 4: Get the index of the most relevant result and cast it to native int
most_relevant_index = int(cosine_sim_matrix.argmax())

# Step 5: Retrieve the most relevant text from the dataset
try:
    most_relevant_text = db["text"][most_relevant_index].numpy()
    if isinstance(most_relevant_text, np.ndarray):
        most_relevant_text = most_relevant_text.item()  # Extract scalar value if it's an array
    #print(f"Most Relevant Result:\n{most_relevant_text}")
except Exception as e:
    print(f"Error retrieving the most relevant result: {str(e)}")

template= """You are very helpful chatbot trying to help the readers for the artcile that they are reading. They are going to ask you to answer their questions based on the information in the article.
You know the follwoing information:

{chunks_formatted}

Answer to the following question from a customer. Use only information from the previous context information. Do not invent stuff.

Question: {query}

Answer:
"""

prompt = PromptTemplate(
    input_variables=["chunks_formatted", "query"],
    template=template
)


# format the prompt
chunks_formatted = "\n\n".join(most_relevant_text)
prompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)

# generate answer
llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0)
answer = llm(prompt_formatted)
print(answer)
